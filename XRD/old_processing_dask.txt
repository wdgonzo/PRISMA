"""
Unified GSAS Processing Module
==============================
This module provides a streamlined approach to X-ray diffraction data processing
using a unified 4D data structure that eliminates the need for data conversions.

Main Components:
- XRDDataset: Unified 4D data structure (peaks, frames, azimuths, measurements)
- Parallel processing with Dask
- Direct integration with visualization
- Efficient I/O with Zarr storage

Author: William Gonzalez, Luke Davenport, Adrian Guzman (Refactored)
Date: September 2025
Version: 1.1 - FIXED
"""

import G2script
import pandas as pd

# GSAS-II Performance Optimization
def optimize_gsas_performance(memory_gb: float = None, cpu_cores: int = None):
    """
    Optimize GSAS-II G2script performance parameters for HPC deployment.

    Args:
        memory_gb: Available memory in GB (auto-detected if None)
        cpu_cores: Number of CPU cores (auto-detected if None)
    """
    import psutil
    import os

    if memory_gb is None:
        memory_gb = psutil.virtual_memory().total / (1024**3)

    if cpu_cores is None:
        cpu_cores = os.cpu_count()

    # Optimize blkSize based on memory and CPU characteristics
    # Higher memory systems can handle larger block sizes
    # Default: 128, but can range from 64 to 1024 based on system specs
    if memory_gb >= 64:  # High-memory HPC node
        optimal_blksize = 2**9  # 512
    elif memory_gb >= 32:  # Medium-memory system
        optimal_blksize = 2**8  # 256
    elif memory_gb >= 16:  # Standard workstation
        optimal_blksize = 2**7  # 128 (default)
    else:  # Low-memory system
        optimal_blksize = 2**6  # 64

    # Set the optimized block size
    G2script.blkSize = optimal_blksize

    # Configure multiprocessing if available
    if hasattr(G2script, 'Multiprocessing_cores'):
        # Use 75% of available cores, leaving some for system processes
        optimal_cores = max(1, int(cpu_cores * 0.75))
        G2script.Multiprocessing_cores = optimal_cores

    # Enable performance timing if available
    if hasattr(G2script, 'Show_timing'):
        G2script.Show_timing = True

    print(f"üöÄ GSAS-II Performance Optimization:")
    print(f"   Memory: {memory_gb:.1f} GB")
    print(f"   CPU cores: {cpu_cores}")
    print(f"   Optimal blkSize: {optimal_blksize} (2^{optimal_blksize.bit_length()-1})")
    if hasattr(G2script, 'Multiprocessing_cores'):
        print(f"   Multiprocessing cores: {getattr(G2script, 'Multiprocessing_cores', 'not available')}")

    return {
        'blkSize': optimal_blksize,
        'memory_gb': memory_gb,
        'cpu_cores': cpu_cores,
        'multiprocessing_cores': getattr(G2script, 'Multiprocessing_cores', None)
    }

# Initialize GSAS optimization on import
_gsas_optimization_info = optimize_gsas_performance()

# Silent mode configuration
import sys
import os
from contextlib import contextmanager
from io import StringIO

@contextmanager
def silent_gsas_operations():
    """
    Context manager to suppress GSAS-II verbose output during operations.
    Redirects stdout to capture GSAS messages while preserving our system messages.
    """
    original_stdout = sys.stdout
    original_stderr = sys.stderr

    # Create string buffers to capture output
    captured_stdout = StringIO()
    captured_stderr = StringIO()

    try:
        # Redirect output streams
        sys.stdout = captured_stdout
        sys.stderr = captured_stderr

        # Set GSAS to minimal output
        G2script.SetPrintLevel("none")

        yield

    finally:
        # Restore original output streams
        sys.stdout = original_stdout
        sys.stderr = original_stderr

        # Optionally process captured output for errors only
        captured_out = captured_stdout.getvalue()
        captured_err = captured_stderr.getvalue()

        # Only print if there are actual errors (not warnings or info)
        if captured_err and ("error" in captured_err.lower() or "failed" in captured_err.lower()):
            print(f"‚ö†Ô∏è GSAS Error: {captured_err.strip()}")

# Global cache for GSAS computations (saves intermediate results)
_gsas_cache = {}

def get_gsas_cache():
    """Get the global GSAS computation cache."""
    return _gsas_cache

def clear_gsas_cache():
    """Clear the global GSAS computation cache."""
    global _gsas_cache
    cache_size = len(_gsas_cache)
    _gsas_cache.clear()
    print(f"üóëÔ∏è Cleared GSAS cache ({cache_size} entries)")

def cache_key_from_params(file_path: str, params: 'GSASParams', frame_index: int) -> str:
    """Generate a cache key from processing parameters."""
    import hashlib

    # Create a hash based on file path, key parameters, and frame
    key_data = f"{file_path}_{params.limits}_{params.spacing}_{params.azimuths}_{frame_index}"
    return hashlib.md5(key_data.encode()).hexdigest()[:16]

def setup_hpc_environment():
    """
    Configure environment variables for optimal HPC performance.

    Sets up thread management, BLAS optimization, and network configuration.
    """
    import os

    # Prevent thread oversubscription (critical for HPC nodes)
    thread_vars = {
        'OMP_NUM_THREADS': '1',          # OpenMP threads
        'MKL_NUM_THREADS': '1',          # Intel MKL threads
        'OPENBLAS_NUM_THREADS': '1',     # OpenBLAS threads
        'NUMEXPR_NUM_THREADS': '1',      # NumExpr threads
        'VECLIB_MAXIMUM_THREADS': '1',   # macOS Accelerate
    }

    print("üñ•Ô∏è HPC Environment Configuration:")
    for var, value in thread_vars.items():
        old_value = os.environ.get(var, 'not set')
        os.environ[var] = value
        print(f"   {var}: {old_value} ‚Üí {value}")

    # Configure NumPy to use optimal BLAS
    try:
        import numpy as np
        config_info = np.show_config()
        print(f"   NumPy BLAS: {_detect_blas_library()}")
    except:
        print("   NumPy BLAS: detection failed")

    # Set Dask configuration for HPC
    try:
        import dask
        dask.config.set({
            'distributed.worker.memory.target': 0.7,    # Use 70% of worker memory
            'distributed.worker.memory.spill': 0.8,     # Spill at 80%
            'distributed.worker.memory.pause': 0.9,     # Pause at 90%
            'distributed.worker.memory.terminate': 0.95,  # Terminate at 95%
            'distributed.comm.compression': 'lz4',      # Fast compression
            'distributed.scheduler.worker-ttl': '5 minutes',  # Worker timeout
        })
        print("   Dask: Configured for HPC memory management")
    except ImportError:
        print("   Dask: Configuration skipped (not available)")

def _detect_blas_library() -> str:
    """Detect which BLAS library NumPy is using."""
    try:
        import numpy as np
        config = np.show_config()

        # Check for common BLAS libraries
        if 'mkl' in str(config).lower():
            return 'Intel MKL (optimal)'
        elif 'openblas' in str(config).lower():
            return 'OpenBLAS (good)'
        elif 'accelerate' in str(config).lower():
            return 'macOS Accelerate (good)'
        elif 'blas' in str(config).lower():
            return 'Generic BLAS (basic)'
        else:
            return 'Unknown BLAS library'
    except:
        return 'BLAS detection failed'

def optimize_numpy_performance():
    """
    Optimize NumPy performance for scientific computing.

    Configures BLAS/LAPACK settings and validates performance.
    """
    import numpy as np

    print("üî¢ NumPy Performance Optimization:")

    # Detect BLAS library
    blas_lib = _detect_blas_library()
    print(f"   BLAS Library: {blas_lib}")

    # Check for multithreading capability
    try:
        from threadpoolctl import threadpool_info
        pools = threadpool_info()

        blas_threads = 0
        for pool in pools:
            if pool.get('api') in ['blas', 'lapack']:
                blas_threads += pool.get('num_threads', 0)

        print(f"   BLAS/LAPACK threads: {blas_threads}")

        # Recommend optimal settings
        if blas_threads > 1:
            print("   ‚ö†Ô∏è  Warning: Multi-threaded BLAS detected")
            print("   üí° For HPC use, set thread environment variables to 1")

    except ImportError:
        print("   Thread info: Install 'threadpoolctl' for detailed analysis")

    # Basic performance validation
    test_size = 1000
    test_array = np.random.random((test_size, test_size)).astype(np.float32)

    import time
    start = time.time()
    _ = np.dot(test_array, test_array)  # Matrix multiplication test
    elapsed = time.time() - start

    # Performance assessment
    operations = test_size**3  # Approximate FLOPs for matrix multiplication
    gflops = operations / elapsed / 1e9

    print(f"   Matrix mult performance: {gflops:.1f} GFLOPS")

    if gflops > 50:
        print("   ‚úÖ Excellent NumPy performance")
    elif gflops > 20:
        print("   ‚úÖ Good NumPy performance")
    elif gflops > 5:
        print("   ‚ö†Ô∏è  Fair NumPy performance - consider BLAS optimization")
    else:
        print("   ‚ùå Poor NumPy performance - BLAS configuration needed")

# Initialize HPC environment on import
setup_hpc_environment()
optimize_numpy_performance()

# Global performance monitor
try:
    from .performance_monitor import PerformanceMonitor
    _performance_monitor = PerformanceMonitor()
    print("üìä Performance monitoring enabled")
except ImportError:
    _performance_monitor = None
    print("üìä Performance monitoring disabled (performance_monitor.py not found)")
import numpy as np
import os
import re
import glob
import json
import sys
from dataclasses import dataclass
import dask.array as da
from dask import delayed, compute
from dask.distributed import Client
from enum import Enum
import logging
from typing import List, Tuple, Dict, Optional, Union
import zarr
from zarr.codecs import BloscCodec, BloscCname, BloscShuffle
import numcodecs


# ================== ENUMS AND CONSTANTS ==================

class Stages(Enum):
    """Enumeration for different experimental stages."""
    BEF = 0
    AFT = 1
    CONT = 2
    DELT = 3
    DELTDSPACING = 4

@dataclass
class PeakConfig:
    """Configuration for peak fitting and analysis."""
    ACTIVE_PEAKS = [
        4.88,  # 110 PEAK
        6.92,  # 200 PEAK   
        8.46   # 211 PEAK
    ]
    
    AVAILABLE_PEAKS = [
        4.16,
        4.38,
        4.47,
        4.70,  # 110
        4.77,  # 110
        4.88,  # 110 PEAK
        5.05,  # 110
        5.30,
        5.36,
        5.50,
        5.62,
        6.92,  # 200 PEAK   
        7.06,
        7.46,
        7.79,  # 200 austenite
        8.13,  # 211
        8.46,  # 211 PEAK
        8.62,  # 211
        8.81,  # 211
        8.97,
        9.13,
        9.54,
        9.76,
    ]
    
    MILLER_INDICES = [110, 200, 211]  # Corresponding Miller indices

    @classmethod
    def get_active_peaks(cls) -> list:
        """Get list of currently active peaks for fitting."""
        return cls.ACTIVE_PEAKS

    @classmethod
    def get_available_peaks(cls) -> list:
        """Get list of all available peaks in the system."""
        return cls.AVAILABLE_PEAKS

    @classmethod
    def get_background_peak_candidates(cls, limits: list) -> list:
        """
        Get peaks that should be treated as background peaks.
        Returns available peaks that are not active and within the analysis range.

        Args:
            limits: 2Œ∏ range limits [min, max]

        Returns:
            List of peak positions to add as background peaks
        """
        active_peaks = set(cls.get_active_peaks())
        available_peaks = cls.get_available_peaks()

        # Find interference candidates: available but not selected for analysis
        interference_peaks = [peak for peak in available_peaks if peak not in active_peaks]

        # Filter to only peaks within the background fitting range
        background_candidates = [
            peak for peak in interference_peaks
            if limits[0] <= peak <= limits[1]
        ]

        return background_candidates

    @classmethod
    def get_miller_indices(cls) -> list:
        """Get Miller indices for active peaks."""
        return cls.MILLER_INDICES

# ================== UNIFIED DATA STRUCTURE ==================

class XRDDataset:
    """
    Unified 4D dataset for XRD data with flexible access patterns.
    Shape: (peaks, frames, azimuths, measurements)
    
    FIXED: Uses numpy arrays internally during construction, converts to dask for computation
    """
    
    def __init__(self, n_peaks: int, n_frames: int, n_azimuths: int, 
                 measurement_cols: List[str], params: 'GSASParams', 
                 chunks: Tuple = None):
        """
        Initialize XRD dataset.
        
        Args:
            n_peaks: Number of diffraction peaks
            n_frames: Number of time/depth frames
            n_azimuths: Number of azimuthal bins
            measurement_cols: List of measurement column names
            params: Processing parameters
            chunks: Dask chunking strategy
        """
        self.n_peaks = n_peaks
        self.n_frames = n_frames  
        self.n_azimuths = n_azimuths
        self.measurement_cols = measurement_cols
        self.n_measurements = len(measurement_cols)
        self.params = params
        
        # Create measurement column mapping
        self.col_idx = {col: i for i, col in enumerate(measurement_cols)}
        
        # Advanced chunking strategy optimized for 100MB target (2025 best practice)
        if chunks is None:
            chunks = self._calculate_optimal_chunks(n_peaks, n_frames, n_azimuths, self.n_measurements)
        self.chunks = chunks

        # Configure compression for Zarr v3 with proper codec
        self.zarr_codec = BloscCodec(
            cname=BloscCname.zstd,          # Use zstd compression (best for scientific data)
            clevel=3,                       # Balanced compression level
            shuffle=BloscShuffle.shuffle,   # Enable shuffling for better compression
            typesize=4,                     # Explicit typesize for float32 data
            blocksize=0                     # Auto-optimize block size
        )
            
        # CRITICAL FIX: Use numpy arrays during construction
        # Will convert to dask after data is populated
        self._numpy_data = np.zeros((n_peaks, n_frames, n_azimuths, self.n_measurements), 
                                   dtype='float32')
        self._numpy_frame_numbers = np.zeros((n_peaks, n_frames), dtype='int32')
        self._numpy_azimuth_angles = np.zeros((n_peaks, n_azimuths), dtype='float32')
        
        # Initialize dask arrays as None (will be set in finalize)
        self.data = None
        self.frame_numbers = None
        self.azimuth_angles = None
        
        # Metadata
        self.peak_miller_indices = np.zeros(n_peaks, dtype='int32')
        
        # Track if we're in construction mode
        self._construction_mode = True

    def _calculate_optimal_chunks(self, n_peaks: int, n_frames: int, n_azimuths: int, n_measurements: int) -> Tuple[int, ...]:
        """
        Calculate optimal chunk sizes targeting ~100MB per chunk (2025 Dask best practice).

        Returns:
            Tuple of chunk sizes for (peaks, frames, azimuths, measurements)
        """
        # Target chunk size in bytes (100MB)
        target_bytes = 100 * 1024 * 1024

        # Size of each element (float32 = 4 bytes)
        element_size = 4

        # Calculate total elements that fit in target size
        target_elements = target_bytes // element_size

        # Always keep peaks as 1 (process one Miller index at a time)
        peak_chunk = 1
        meas_chunk = n_measurements  # Keep all measurements together

        # Calculate frame and azimuth chunks to reach target
        remaining_elements = target_elements // (peak_chunk * meas_chunk)

        # Determine optimal frame and azimuth distribution
        if remaining_elements >= n_frames * n_azimuths:
            # Small dataset - can fit everything in one chunk
            frame_chunk = n_frames
            az_chunk = n_azimuths
        else:
            # Balance between frame and azimuth chunks
            # Prefer larger frame chunks for better sequential access
            aspect_ratio = n_frames / n_azimuths if n_azimuths > 0 else 1

            if aspect_ratio >= 1:  # More frames than azimuths
                # Prioritize frame chunks
                frame_chunk = min(n_frames, int((remaining_elements * 0.7) ** 0.5 * aspect_ratio))
                frame_chunk = max(1, min(frame_chunk, n_frames))
                az_chunk = min(n_azimuths, remaining_elements // frame_chunk)
                az_chunk = max(1, az_chunk)
            else:  # More azimuths than frames
                # Balance more evenly
                az_chunk = min(n_azimuths, int((remaining_elements * 0.7) ** 0.5 / aspect_ratio))
                az_chunk = max(1, min(az_chunk, n_azimuths))
                frame_chunk = min(n_frames, remaining_elements // az_chunk)
                frame_chunk = max(1, frame_chunk)

        # Ensure chunks don't exceed array dimensions
        frame_chunk = min(frame_chunk, n_frames)
        az_chunk = min(az_chunk, n_azimuths)

        chunks = (peak_chunk, frame_chunk, az_chunk, meas_chunk)

        # Calculate actual chunk size for reporting
        chunk_elements = peak_chunk * frame_chunk * az_chunk * meas_chunk
        chunk_mb = chunk_elements * element_size / (1024 * 1024)

        print(f"üìä Optimized Chunk Strategy (100MB target):")
        print(f"   Target: 100MB, Actual: {chunk_mb:.1f}MB per chunk")
        print(f"   Chunks: {chunks}")
        print(f"   Elements per chunk: {chunk_elements:,}")

        return chunks
    
    def set_frame_data(self, peak_idx: int, frame_idx: int, df: pd.DataFrame):
        """Set entire frame data from DataFrame - FIXED version"""
        if not self._construction_mode:
            raise RuntimeError("Cannot modify data after finalization. Data is immutable once converted to Dask.")
        
        # Sort dataframe by azimuth to ensure consistent ordering
        df_sorted = df.sort_values('azimuth').reset_index(drop=True)
        
        # Create azimuth mapping for this frame
        azimuth_to_idx = {}
        for row_idx, row in df_sorted.iterrows():
            azimuth = row['azimuth']
            az_idx = self._azimuth_to_index(azimuth)
            azimuth_to_idx[azimuth] = az_idx
            
            # Set azimuth angle (only need to do once per azimuth)
            if self._numpy_azimuth_angles[peak_idx, az_idx] == 0:
                self._numpy_azimuth_angles[peak_idx, az_idx] = azimuth
        
        # Set frame number (once per frame)
        if 'frame' in df_sorted.columns and len(df_sorted) > 0:
            self._numpy_frame_numbers[peak_idx, frame_idx] = df_sorted.iloc[0]['frame']
        
        # Set measurement data for each row
        for _, row in df_sorted.iterrows():
            azimuth = row['azimuth']
            az_idx = azimuth_to_idx[azimuth]
            
            # Set each measurement value
            for col in self.measurement_cols:
                if col in row:
                    value = row[col]
                    if not pd.isna(value):
                        col_idx = self.col_idx[col]
                        self._numpy_data[peak_idx, frame_idx, az_idx, col_idx] = value
    
    def _azimuth_to_index(self, azimuth: float) -> int:
        """Convert azimuth angle to array index - FIXED version"""
        # Calculate index based on spacing and start angle
        start_azimuth = self.params.azimuths[0]
        spacing = self.params.spacing
        
        # Round azimuth to nearest spacing increment
        azimuth_rounded = round(azimuth / spacing) * spacing
        index = int((azimuth_rounded - start_azimuth) / spacing)
        
        # Ensure index is within bounds
        return max(0, min(index, self.n_azimuths - 1))
    
    def finalize(self):
        """Convert numpy arrays to dask arrays after construction is complete"""
        if self._construction_mode:
            print("Finalizing dataset - converting to Dask arrays...")
            
            # Convert numpy arrays to dask
            self.data = da.from_array(self._numpy_data, chunks=self.chunks)
            self.frame_numbers = da.from_array(self._numpy_frame_numbers, 
                                              chunks=(self.chunks[0], self.chunks[1]))
            self.azimuth_angles = da.from_array(self._numpy_azimuth_angles,
                                               chunks=(self.chunks[0], self.chunks[2]))
            
            # Report statistics
            non_zero = np.count_nonzero(self._numpy_data)
            total = self._numpy_data.size
            print(f"Dataset finalized: {non_zero}/{total} non-zero values ({100*non_zero/total:.2f}%)")
            
            # Clear construction mode flag
            self._construction_mode = False
    
    # ============ FLEXIBLE ACCESS PATTERNS ============
    
    def get_peak(self, peak_idx: int) -> da.Array:
        """Get all data for a specific peak"""
        if self.data is None:
            self.finalize()
        return self.data[peak_idx]
    
    def get_frame(self, peak_idx: int, frame_idx: int) -> da.Array:
        """Get specific frame data"""
        if self.data is None:
            self.finalize()
        return self.data[peak_idx, frame_idx]
    
    def get_azimuth_timeseries(self, peak_idx: int, azimuth_idx: int) -> da.Array:
        """Get time series for specific peak/azimuth"""
        if self.data is None:
            self.finalize()
        return self.data[peak_idx, :, azimuth_idx]
    
    def get_measurement(self, measurement: str) -> da.Array:
        """Get specific measurement across all dimensions"""
        if self.data is None:
            self.finalize()
        col_idx = self.col_idx[measurement]
        return self.data[:, :, :, col_idx]
    
    def get_peak_measurement(self, peak_idx: int, measurement: str) -> da.Array:
        """Get specific measurement for one peak"""
        if self.data is None:
            self.finalize()
        col_idx = self.col_idx[measurement]
        return self.data[peak_idx, :, :, col_idx]
    
    # ============ COMPUTATIONAL METHODS ============
    
    def calculate_delta(self, measurement: str, axis: int = 1) -> None:
        """Calculate differences along specified axis and add as new measurement"""
        if self.data is None:
            self.finalize()
            
        col_idx = self.col_idx[measurement]
        data_slice = self.data[:, :, :, col_idx]
        delta_values = da.diff(data_slice, axis=axis, prepend=0)
        
        delta_name = f'delta {measurement}'
        self.add_measurement(delta_name, delta_values)
    
    def calculate_strain(self, reference_d: np.ndarray) -> None:
        """Calculate strain and add as new measurement - FIXED version"""
        if self._construction_mode:
            # Work with numpy arrays during construction
            d_col_idx = self.col_idx['d']
            d_values = self._numpy_data[:, :, :, d_col_idx]
            
            # Broadcast reference_d to match data dimensions
            if reference_d.ndim == 2:  # (peaks, azimuths)
                # Need to broadcast to (peaks, frames, azimuths)
                reference_d_broadcast = np.broadcast_to(
                    reference_d[:, np.newaxis, :], 
                    (self.n_peaks, self.n_frames, self.n_azimuths)
                ).copy()  # Make a copy to ensure it's writable
            else:
                reference_d_broadcast = reference_d
            
            # Calculate strain only where both values are non-zero
            strain = np.zeros_like(d_values)
            
            # Create masks separately for each array
            d_mask = (d_values != 0) & ~np.isnan(d_values)
            ref_mask = (reference_d_broadcast != 0) & ~np.isnan(reference_d_broadcast)
            combined_mask = d_mask & ref_mask
            
            # Apply strain calculation only where mask is true
            strain[combined_mask] = (d_values[combined_mask] - reference_d_broadcast[combined_mask]) / reference_d_broadcast[combined_mask]
            
            # Add strain as new measurement
            new_data = np.concatenate([self._numpy_data, strain[..., np.newaxis]], axis=-1)
            self._numpy_data = new_data
            self.col_idx['strain'] = self.n_measurements
            self.measurement_cols.append('strain')
            self.n_measurements += 1
            
            # Also add absolute strain
            abs_strain = np.abs(strain)
            new_data = np.concatenate([self._numpy_data, abs_strain[..., np.newaxis]], axis=-1)
            self._numpy_data = new_data
            self.col_idx['abs strain'] = self.n_measurements
            self.measurement_cols.append('abs strain')
            self.n_measurements += 1
            
        else:
            # Work with dask arrays after finalization
            d_col_idx = self.col_idx['d']
            d_values = self.data[:, :, :, d_col_idx]
            
            # Broadcast reference_d to match data dimensions
            if reference_d.ndim == 2:  # (peaks, azimuths)
                # Broadcast to match d_values shape
                reference_d_broadcast = np.broadcast_to(
                    reference_d[:, np.newaxis, :],
                    (self.n_peaks, self.n_frames, self.n_azimuths)
                )
            else:
                reference_d_broadcast = reference_d
                
            # Convert to dask array
            reference_d_da = da.from_array(reference_d_broadcast, chunks=self.chunks[:3])
            
            # Calculate strain with proper masking
            # Use da.where to handle division by zero
            strain = da.where(
                (reference_d_da != 0) & (d_values != 0),
                (d_values - reference_d_da) / reference_d_da,
                0  # Set strain to 0 where we can't calculate it
            )
            
            self.add_measurement('strain', strain)
            
            # Add absolute strain
            abs_strain = da.abs(strain)
            self.add_measurement('abs strain', abs_strain)
    
    def add_measurement(self, name: str, values: Union[da.Array, np.ndarray]):
        """Add a new measurement to the dataset"""
        if self.data is None:
            self.finalize()
            
        if name not in self.col_idx:
            # Ensure values have correct shape
            if values.shape[:3] != self.data.shape[:3]:
                raise ValueError(f"New measurement shape {values.shape} incompatible with dataset shape {self.data.shape[:3]}")
            
            # Expand data array
            values_expanded = values[..., np.newaxis] if values.ndim == 3 else values
            
            # Convert numpy to dask if needed
            if isinstance(values_expanded, np.ndarray):
                values_expanded = da.from_array(values_expanded, chunks=self.chunks)
            
            new_data = da.concatenate([self.data, values_expanded], axis=-1)
            self.data = new_data
            
            # Update metadata
            self.col_idx[name] = self.n_measurements
            self.measurement_cols.append(name)
            self.n_measurements += 1
    
    # ============ I/O METHODS - FIXED ============
    
    def save(self, path: str):
        """Save to Zarr format - FIXED version"""
        os.makedirs(path, exist_ok=True)
        
        # Ensure data is finalized
        if self.data is None:
            self.finalize()
        
        print(f"Saving to {path}...")

        # Save main data with Zarr v3 codec optimization
        zarr_version = getattr(zarr, '__version__', '3.0.0')

        if zarr_version.startswith('3'):
            # Use proper v3 codec pipeline - BloscCodec with BytesCodec
            print(f"Using Zarr v3 ({zarr_version}) with optimized BloscCodec...")
            from zarr.codecs import BytesCodec
            # For Zarr v3: BloscCodec needs to be paired with BytesCodec for the complete pipeline
            zarr_kwargs = {'codecs': [BytesCodec(), self.zarr_codec]}
        else:
            # Fallback to v2 compressor syntax for older versions
            print(f"Using Zarr v2 ({zarr_version}) with compressor fallback...")
            # Convert BloscCodec to numcodecs.Blosc for v2 compatibility
            fallback_codec = numcodecs.Blosc(cname='zstd', clevel=3, shuffle=numcodecs.Blosc.SHUFFLE)
            zarr_kwargs = {'compressor': fallback_codec}

        # Save all arrays with consistent compression
        self.data.to_zarr(f"{path}/data.zarr", overwrite=True, **zarr_kwargs)
        self.frame_numbers.to_zarr(f"{path}/frame_numbers.zarr", overwrite=True, **zarr_kwargs)
        self.azimuth_angles.to_zarr(f"{path}/azimuth_angles.zarr", overwrite=True, **zarr_kwargs)
        
        # Save metadata
        metadata = {
            'n_peaks': self.n_peaks,
            'n_frames': self.n_frames,
            'n_azimuths': self.n_azimuths,
            'measurement_cols': self.measurement_cols,
            'col_idx': self.col_idx,
            'peak_miller_indices': self.peak_miller_indices.tolist(),
            'params': {
                'sample': self.params.sample,
                'stage': self.params.stage.name,
                'spacing': self.params.spacing,
                'azimuths': self.params.azimuths,
                'frames': self.params.frames
            }
        }
        
        with open(f"{path}/metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"Saved successfully!")
    
    @classmethod
    def load(cls, path: str, params: 'GSASParams'):
        """Load from Zarr format - FIXED version"""
        # Load metadata
        with open(f"{path}/metadata.json", 'r') as f:
            metadata = json.load(f)
        
        # Create instance
        instance = cls(metadata['n_peaks'], metadata['n_frames'], 
                      metadata['n_azimuths'], metadata['measurement_cols'], params)
        
        # Skip construction mode, load directly as dask
        instance._construction_mode = False
        
        # Load data arrays
        instance.data = da.from_zarr(f"{path}/data.zarr")
        instance.frame_numbers = da.from_zarr(f"{path}/frame_numbers.zarr")
        instance.azimuth_angles = da.from_zarr(f"{path}/azimuth_angles.zarr")
        
        # Load metadata
        instance.col_idx = metadata['col_idx']
        instance.peak_miller_indices = np.array(metadata['peak_miller_indices'])
        
        return instance
    
    # ============ CONVERSION FOR VISUALIZATION ============
    
    def to_visualization_dataframe(self, peak_idx: int, measurement: str) -> pd.DataFrame:
        """
        Convert specific peak and measurement to DataFrame for visualization.
        Returns DataFrame with columns: frame, azimuth, value
        """
        if measurement not in self.col_idx:
            raise ValueError(f"Measurement '{measurement}' not found in dataset")
        
        if self.data is None:
            self.finalize()
        
        col_idx = self.col_idx[measurement]
        peak_data = self.data[peak_idx, :, :, col_idx].compute()
        
        # Create coordinate arrays
        frames = np.arange(self.n_frames)
        azimuths = np.linspace(self.params.azimuths[0], self.params.azimuths[1], self.n_azimuths)
        
        # Create meshgrid and flatten
        frame_grid, azimuth_grid = np.meshgrid(frames, azimuths, indexing='ij')
        
        df = pd.DataFrame({
            'frame': frame_grid.flatten(),
            'azimuth': azimuth_grid.flatten(),
            measurement: peak_data.flatten()
        })
        
        # Remove NaN values
        df = df.dropna()
        
        return df


# ================== PROCESSING PARAMETERS ==================

@dataclass
class GSASParams:
    """Parameters for GSAS processing configuration."""
    # File paths
    image_folder: str
    control_file: str
    mask_file: str
    
    # Sample information
    sample: str
    setting: str
    stage: Stages
    miller: int
    notes: str
    
    # Analysis parameters
    limits: tuple[float, float]
    backgrounds: tuple[float, float]
    azimuths: tuple[float, float]
    frames: tuple[int, int]
    num_peaks: int
    
    # Processing parameters
    spacing: int
    step: int
    air_frames: int = 0
    
    def filename(self) -> str:
        """Generate base filename for this dataset."""
        return f"{self.sample}-{self.stage.name}"
    
    def total_angle(self) -> int:
        """Calculate total azimuthal angle range."""
        return self.azimuths[-1] - self.azimuths[0]
    
    def image_file(self) -> str:
        """Generate path to image files."""
        return f"{self.image_folder}/Images/{self.setting}-{self.sample}-{self.stage.name}"
    
    def save_path(self) -> str:
        """Generate path for saving processed data."""
        return f"{self.image_folder}/Zarr/{self.setting}-{self.sample}-{self.stage.name}-{self.spacing}-{self.step}"
    
    def ref_file(self) -> str:
        """Generate path to reference files."""
        return f"{self.image_folder}/Refs/{self.setting}"


# ================== PROCESSING FUNCTIONS ==================

@delayed
def gsas_parallel(file: str, params: GSASParams, frame_index: int,
                  references: Optional[np.ndarray] = None) -> List[pd.DataFrame]:
    """
    Process a single diffraction image using GSAS-II (parallelized).

    ENHANCED: Includes caching, performance monitoring, and optimization
    """

    # Check cache first
    cache_key = cache_key_from_params(file, params, frame_index)
    cache = get_gsas_cache()

    if cache_key in cache:
        print(f"üéØ Cache hit for frame {frame_index}")
        return cache[cache_key]

    # Performance monitoring wrapper
    if _performance_monitor:
        with _performance_monitor.monitor_operation(f"gsas_frame_{frame_index}"):
            return _process_single_frame(file, params, frame_index, references, cache_key, cache)
    else:
        return _process_single_frame(file, params, frame_index, references, cache_key, cache)

def _process_single_frame(file: str, params: GSASParams, frame_index: int,
                         references: Optional[np.ndarray], cache_key: str, cache: dict) -> List[pd.DataFrame]:
    """Internal function to process a single frame with caching."""

    # Load analysis settings
    with open('submitted_values.json', 'r') as f:
        values = json.load(f)

    # Use silent mode for all GSAS operations
    with silent_gsas_operations():
        # Initialize GSAS project
        project = G2script.G2Project(newgpx=f"{params.filename()}-Frame{frame_index}.gpx")

        # Load and configure image
        project.add_image(file)
        image = project.images()[0]

        # Cache frequently used params
        limits = params.limits
        spacing = params.spacing
        az0 = params.azimuths[0]

        # Apply detector configuration
        image.loadControls(params.control_file)
        image.setControl('pixelSize', [172, 172])
        image.setControl('IOtth', [limits[0] - 1, limits[-1] + 1])
        image.setControl('outChannels', 1000)

        # Configure azimuthal integration
        if params.azimuths[0] != 0 or params.azimuths[-1] != 360:
            image.setControl('fullIntegrate', False)
            image.setControl('LRazimuth', params.azimuths)
        image.setControl('outAzimuths', (params.total_angle() / spacing))

        # Apply mask and recalibrate
        image.GeneratePixelMask()
        image.Recalibrate()

        # Perform 2D to 1D integration
        az_map = image.IntThetaAzMap()
        az_histos = image.Integrate(ThetaAzimMap=az_map)
    
    print(f"‚úÖ Setup complete for frame {frame_index}")
    
    # Process each azimuthal slice with silent GSAS operations
    peak_results = []
    active_peaks = PeakConfig.get_active_peaks()

    # Get background peak candidates once (same for all azimuthal slices)
    background_candidates = PeakConfig.get_background_peak_candidates(limits)

    if background_candidates:
        print(f"   üéØ Found {len(background_candidates)} background peak candidates: {background_candidates}")

    # Initialize results for each peak
    for _ in range(len(active_peaks)):
        peak_results.append([])

    # Wrap all peak fitting operations in silent mode
    with silent_gsas_operations():
        for histo_index, current_histo in enumerate(az_histos):
            # Set basic refinements first (without background peak flags)
            current_histo.set_refinements({
                "Limits": limits,
                "Background": {
                    "no. coeffs": 2,
                    'type': 'chebyschev-1',
                    "refine": True
                }
            })

            # Add background peaks first (before setting background peak refinement flags)
            if background_candidates:
                for bg_peak in background_candidates:
                    # Add as background peak with reasonable initial parameters
                    # position, intensity, width, gamma
                    current_histo.add_background_peak(bg_peak, 1000.0, 0.1, 0.1)

            # Add analysis peaks
            for peak in active_peaks:
                current_histo.add_peak(1, ttheta=peak)

            # Now update refinements to include background peak refinement (after peaks are added)
            if background_candidates:
                background_peak_flags = [[i, True] for i in range(len(background_candidates))]
                current_histo.set_refinements({
                    "Limits": limits,
                    "Background": {
                        "no. coeffs": 2,
                        'type': 'chebyschev-1',
                        "refine": True,
                        "peaks": background_peak_flags
                    }
                })

            # Use advanced pybaselines background optimization
            try:
                # Use iARPLS algorithm for better background fitting
                current_histo.calc_autobkg(opt=1)  # opt=1 for iARPLS (improved ARPLS)
            except Exception:
                # Fallback to standard background calculation
                current_histo.calc_autobkg(1, 9)

            # Background recalculation after adding peaks
            try:
                current_histo.calc_autobkg(opt=1)
            except Exception:
                current_histo.calc_autobkg(1, 9)

            # Perform peak fitting
            if values["analyze option"] == "Accurate":
                _perform_accurate_refinement(current_histo, params, histo_index)
            else:
                _perform_fast_refinement(current_histo, params, histo_index)

            # Background optimization between refinements
            try:
                current_histo.calc_autobkg(opt=1)
            except Exception:
                current_histo.calc_autobkg(1, 9)

            # Perform second refinement
            if values["analyze option"] == "Accurate":
                _perform_accurate_refinement(current_histo, params, histo_index)
            else:
                _perform_fast_refinement(current_histo, params, histo_index)

            # Extract peak parameters and create dataframes
            for peak_index, peakVals in enumerate(current_histo.PeakList):
                new_frame = pd.DataFrame([peakVals], columns=['pos', 'area', 'sigma', 'gamma'])
                new_frame['azimuth'] = (spacing * histo_index) + az0
                new_frame['frame'] = frame_index
                new_frame['d'] = (0.1729) / (2 * np.sin(np.deg2rad(new_frame['pos'] / 2)))

                # Calculate strain if references provided (as numpy array)
                if references is not None and peak_index < references.shape[0]:
                    # Get reference d for this peak and azimuth
                    az_idx = min(histo_index, references.shape[1] - 1)
                    ref_d = references[peak_index, az_idx]
                    if ref_d != 0 and not np.isnan(ref_d):
                        new_frame['strain'] = (new_frame['d'] - ref_d) / ref_d
                    else:
                        new_frame['strain'] = np.nan

                peak_results[peak_index].append(new_frame)

        image.clearPixelMask()
    
    # Concatenate results for each peak
    working_histos = [pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()
                      for frames in peak_results]

    # Cache the results for future use
    cache[cache_key] = working_histos
    print(f"üíæ Cached results for frame {frame_index}")

    return working_histos


def _perform_accurate_refinement(histo, params, histo_index):
    """Perform detailed iterative peak refinement with convergence detection and validation."""
    az_angle = ((params.spacing) * histo_index) + params.azimuths[0]
    sample_id = f"{params.sample}-{az_angle}"

    # Note: Instrument parameter optimization would be done via G2pwd.setPeakInstPrmMode(False)
    # but this requires direct GSAS-II module access

    # Parameter bounds for validation
    MIN_AREA = 1e-6
    MAX_AREA = 1e8
    MIN_SIGMA = 0.001  # Minimum peak width
    MAX_SIGMA = 2.0    # Maximum reasonable peak width
    MIN_GAMMA = 0.0    # Lorentzian component
    MAX_GAMMA = 1.0
    CONVERGENCE_THRESHOLD = 1e-4  # Relative change threshold

    def _validate_parameters(peak_list):
        """Validate that all peak parameters are physically reasonable."""
        for i, peak in enumerate(peak_list):
            pos, area, sigma, gamma = peak

            # Check bounds
            if not (MIN_AREA <= area <= MAX_AREA):
                return False, f"Peak {i}: area {area} out of bounds [{MIN_AREA}, {MAX_AREA}]"
            if not (MIN_SIGMA <= sigma <= MAX_SIGMA):
                return False, f"Peak {i}: sigma {sigma} out of bounds [{MIN_SIGMA}, {MAX_SIGMA}]"
            if not (MIN_GAMMA <= gamma <= MAX_GAMMA):
                return False, f"Peak {i}: gamma {gamma} out of bounds [{MIN_GAMMA}, {MAX_GAMMA}]"

            # Check for reasonable position (within 2-theta limits)
            if not (params.limits[0] <= pos <= params.limits[1]):
                return False, f"Peak {i}: position {pos} outside limits {params.limits}"

        return True, "All parameters valid"

    def _calculate_parameter_changes(old_params, new_params):
        """Calculate relative changes in parameters for convergence detection."""
        if not old_params or not new_params:
            return float('inf')

        max_rel_change = 0.0
        for old_peak, new_peak in zip(old_params, new_params):
            for old_val, new_val in zip(old_peak, new_peak):
                if abs(old_val) > 1e-10:  # Avoid division by very small numbers
                    rel_change = abs((new_val - old_val) / old_val)
                    max_rel_change = max(max_rel_change, rel_change)

        return max_rel_change

    def _safe_refine(flags, step_name):
        """Helper function for safe peak refinement with logging"""
        try:
            histo.set_peakFlags(**flags)
            # Use 'hold' mode to prevent instrument parameter refinement for speed
            histo.refine_peaks(mode='hold')
            return True
        except Exception as e:
            print(f"{step_name} failed for {sample_id}: {str(e)}")
            return False

    # Store previous parameters for convergence detection
    previous_params = None

    # Initial area-only refinement
    if not _safe_refine({'area': True}, 'Area refinement'):
        return False

    # Validate initial fit
    valid, msg = _validate_parameters(histo.PeakList)
    if not valid:
        print(f"Initial fit validation failed for {sample_id}: {msg}")
        return False

    # Full parameter refinement
    full_flags = {'area': True, 'pos': True, 'sig': True, 'gam': True}
    if not _safe_refine(full_flags, 'Full refinement'):
        return False

    # Iterative refinement sequence with convergence detection
    refinement_sequence = [
        (full_flags, 'Full'),
        ({'area': False, 'pos': False, 'sig': True, 'gam': False}, 'Sigma'),
        ({'area': False, 'pos': False, 'sig': False, 'gam': True}, 'Gamma'),
        (full_flags, 'Final')
    ]

    max_iterations = 5
    for iteration in range(max_iterations):
        # Store parameters before refinement
        previous_params = [list(peak) for peak in histo.PeakList]

        # Perform refinement sequence
        sequence_success = True
        for flags, step in refinement_sequence:
            if not _safe_refine(flags, f'{step} refinement (iteration {iteration})'):
                sequence_success = False
                break

        if not sequence_success:
            return False

        # Validate parameters after refinement
        valid, msg = _validate_parameters(histo.PeakList)
        if not valid:
            print(f"Parameter validation failed for {sample_id} at iteration {iteration}: {msg}")
            return False

        # Check for convergence
        current_params = [list(peak) for peak in histo.PeakList]
        param_change = _calculate_parameter_changes(previous_params, current_params)

        if param_change < CONVERGENCE_THRESHOLD:
            print(f"Converged for {sample_id} at iteration {iteration} (change: {param_change:.2e})")
            return True

    # Final validation even if max iterations reached
    valid, msg = _validate_parameters(histo.PeakList)
    if not valid:
        print(f"Final validation failed for {sample_id}: {msg}")
        return False

    print(f"Refinement completed for {sample_id} after {max_iterations} iterations")
    return True


def _perform_fast_refinement(histo, params, histo_index):
    """Perform quick peak refinement optimized for speed with basic validation."""
    az_angle = ((params.spacing) * histo_index) + params.azimuths[0]
    sample_id = f"{params.sample}-{az_angle}"

    # Note: Would use G2pwd.setPeakInstPrmMode(False) for speed optimization
    # but requires direct GSAS-II module access

    def _basic_validate(peak_list):
        """Basic validation to catch obvious nonsense values."""
        for i, peak in enumerate(peak_list):
            pos, area, sigma, gamma = peak
            if area <= 0 or sigma <= 0 or gamma < 0:
                return False
            if not (params.limits[0] <= pos <= params.limits[1]):
                return False
        return True

    # Optimized refinement sequence
    refinement_steps = [
        {'area': True, 'pos': True, 'sig': True, 'gam': True},
        {'area': False, 'pos': False, 'sig': True, 'gam': True},
        {'area': True, 'pos': True, 'sig': False, 'gam': False},
        {'area': True, 'pos': True, 'sig': True, 'gam': True},
    ]

    for step_num, flags in enumerate(refinement_steps, 1):
        try:
            histo.set_peakFlags(**flags)
            # Use 'hold' mode for maximum speed
            histo.refine_peaks(mode='hold')
        except Exception as e:
            print(f"Fast refinement step {step_num} failed for {sample_id}: {str(e)}")
            return False

    # Basic validation after refinement
    if not _basic_validate(histo.PeakList):
        print(f"Fast refinement validation failed for {sample_id}")
        return False

    return True


def process_images(params: GSASParams) -> XRDDataset:
    """
    Process all images and return unified XRD dataset.
    
    FIXED: Properly handles strain calculation and dataset finalization
    """
    
    # Process reference images
    ref_tasks = []
    ref_files = []
    for ref_root, dirs, files in os.walk(params.ref_file()):
        for i, ref_imgFile in enumerate(sorted(files)):
            ref_name = os.path.join(ref_root, ref_imgFile)
            if os.path.exists(ref_name) and ref_name.endswith(".tif"):
                ref_task = gsas_parallel(ref_name, params, i, references=None)
                ref_tasks.append(ref_task)
                ref_files.append(ref_name)
    
    if not ref_tasks:
        raise FileNotFoundError("No reference images found")
    
    print(f"Processing {len(ref_tasks)} reference images...")
    ref_results = compute(*ref_tasks)
    
    # Calculate average reference d-spacings
    refs_per_peaks = [list(x) for x in zip(*ref_results)]
    n_peaks = len(refs_per_peaks)
    n_azimuths = int(params.total_angle() / params.spacing)
    
    # Create numpy array for reference d-spacings (peaks x azimuths)
    reference_d_array = np.zeros((n_peaks, n_azimuths), dtype='float32')
    
    for peak_idx, ref_peak in enumerate(refs_per_peaks):
        for az_idx in range(n_azimuths):
            values = []
            for df in ref_peak:
                if len(df) > az_idx and 'd' in df.columns:
                    d_val = df.iloc[az_idx]['d']
                    if not np.isnan(d_val):
                        values.append(d_val)
            
            if values:
                reference_d_array[peak_idx, az_idx] = np.mean(values)
    
    print(f"Calculated reference d-spacings: {np.count_nonzero(reference_d_array)} non-zero values")
    
    # Process sample images
    sample_tasks = []
    sample_files = []
    
    for root, dirs, files in os.walk(params.image_file()):
        files = sorted(files)
        total_frames = len(files)
        final_frame = total_frames - params.air_frames if params.frames[1] == -1 else params.frames[1]
        
        frame_count = 0
        for i, imgFile in enumerate(files):
            name = os.path.join(root, imgFile)
            if (os.path.exists(name) and name.endswith(".tif") and 
                i % params.step == 0 and params.frames[0] <= i < final_frame):
                
                # Pass the reference array
                task = gsas_parallel(name, params, i, references=reference_d_array)
                sample_tasks.append(task)
                sample_files.append(name)
                frame_count += 1
    
    if not sample_tasks:
        raise FileNotFoundError("No sample images found")
    
    print(f"Processing {len(sample_tasks)} sample images...")
    sample_results = compute(*sample_tasks)
    
    # Determine actual dimensions
    n_peaks = len(sample_results[0]) if sample_results else 0
    n_frames = len(sample_results)
    n_azimuths = int(params.total_angle() / params.spacing)
    
    # Define measurement columns (without strain initially)
    measurement_cols = ['pos', 'area', 'sigma', 'gamma', 'd']
    
    # Create XRD dataset
    print("Creating unified dataset...")
    xrd_dataset = XRDDataset(n_peaks, n_frames, n_azimuths, measurement_cols, params)
    
    # Fill dataset with processed data
    for frame_idx, frame_results in enumerate(sample_results):
        for peak_idx, peak_df in enumerate(frame_results):
            if not peak_df.empty:
                xrd_dataset.set_frame_data(peak_idx, frame_idx, peak_df)
    
    # Calculate strain using the reference d-spacings array (before finalization)
    print("Calculating strain...")
    if 'd' in xrd_dataset.col_idx and np.any(reference_d_array != 0):
        xrd_dataset.calculate_strain(reference_d_array)
    
    # CRITICAL: Finalize the dataset to convert to Dask
    xrd_dataset.finalize()
    
    # Calculate derived measurements (after finalization)
    print("Calculating derived measurements...")
    
    # Calculate deltas for various parameters
    for measurement in ['d', 'strain', 'area', 'sigma', 'gamma', 'pos']:
        if measurement in xrd_dataset.col_idx:
            xrd_dataset.calculate_delta(measurement)
    
    # Update Miller indices
    miller_indices = PeakConfig.get_miller_indices()
    xrd_dataset.peak_miller_indices[:len(miller_indices)] = miller_indices
    
    print("Dataset creation complete!")
    return xrd_dataset


def load_or_process_data(params: GSASParams) -> XRDDataset:
    """
    Load existing data or process new data if not found or if force reprocessing.
    
    Args:
        params: Processing parameters
        
    Returns:
        XRDDataset object
    """
    save_path = params.save_path()
    
    # Load analysis settings to check force_reprocess flag
    with open('submitted_values.json', 'r') as f:
        values = json.load(f)
    force_reprocess = values.get('force_reprocess', False)
    
    if not force_reprocess:
        try:
            # Try to load existing data
            print(f"Attempting to load data from {save_path}...")
            dataset = XRDDataset.load(save_path, params)
            print("Successfully loaded existing dataset!")
            return dataset
        except (FileNotFoundError, OSError):
            print("No existing data found. Processing images...")
    else:
        print("Force reprocessing enabled. Processing images...")
    
    # Process new data
    dataset = process_images(params)
    
    # Save processed data
    print(f"Saving processed data to {save_path}...")
    dataset.save(save_path)
    print("Data saved successfully!")
    
    return dataset


def subtract_datasets(before: XRDDataset, after: XRDDataset, 
                     measurements: List[str], shift_val: int = 0) -> XRDDataset:
    """
    Calculate differences between before and after datasets.
    
    Args:
        before: Data from before treatment
        after: Data from after treatment  
        measurements: List of measurements to calculate differences for
        shift_val: Shift to apply for alignment
        
    Returns:
        XRDDataset containing difference data
    """
    # Ensure both datasets are finalized
    if before.data is None:
        before.finalize()
    if after.data is None:
        after.finalize()
    
    # Create new dataset for differences
    diff_params = after.params
    diff_params.stage = Stages.DELT
    
    # Create measurement columns for differences
    diff_measurement_cols = after.measurement_cols.copy()
    for measurement in measurements:
        if measurement in after.col_idx:
            diff_measurement_cols.append(f'diff {measurement}')
    
    # Create difference dataset
    diff_dataset = XRDDataset(after.n_peaks, after.n_frames, after.n_azimuths, 
                             diff_measurement_cols, diff_params)
    
    # Copy existing data from after dataset
    diff_dataset.data = after.data.copy()
    diff_dataset.finalize()
    
    # Calculate differences
    for measurement in measurements:
        if measurement in before.col_idx and measurement in after.col_idx:
            before_data = before.get_measurement(measurement)
            after_data = after.get_measurement(measurement)
            
            # Apply shift if specified
            if shift_val != 0:
                if shift_val > 0:
                    # Shift after data down
                    after_data = da.roll(after_data, shift_val, axis=1)
                    after_data[:, :shift_val] = da.nan
                else:
                    # Shift before data down  
                    before_data = da.roll(before_data, -shift_val, axis=1)
                    before_data[:, :(-shift_val)] = da.nan
            
            # Calculate difference
            diff_data = after_data - before_data
            diff_dataset.add_measurement(f'diff {measurement}', diff_data)
    
    return diff_dataset


# ================== MAIN EXECUTION ==================

def main():
    """Example usage of the unified processing module."""
    client = Client()
    
    # Define processing parameters
    parameters = GSASParams(
        image_folder="G:/Data/Feb2025/pilatus/B",
        control_file="G:/Data/Feb2025/pilatus/Ceria/Ceria-Profile.imctrl", 
        mask_file="G:/Data/Feb2025/pilatus/Ceria/test_mask.immask",
        
        sample="B3",
        setting="Profile",
        stage=Stages.BEF,
        miller=0,
        notes="",
        
        limits=[7.2, 9.0],
        backgrounds=[7.2, 9.0], 
        azimuths=[0, 360],
        frames=[0, -1],
        num_peaks=7,
        
        spacing=5,
        step=1
    )
    
    # Load or process data
    dataset = load_or_process_data(parameters)
    
    print("Processing complete!")
    print(f"Dataset shape: {dataset.data.shape}")
    print(f"Available measurements: {dataset.measurement_cols}")
    
    client.close()


if __name__ == "__main__":
    main()