#!/bin/bash
################################################################################
# Crux Production Job Script
# ===========================
# Large-scale XRD batch processing on ALCF Crux supercomputer
#
# Queue: workq-route (1-184 nodes, up to 24 hours)
# Purpose: Production processing of large datasets
#
# This script processes recipe JSON files using Dask-MPI parallelization:
# - Reads recipes from Params/recipes/ or XRD/recipes/
# - Performs GSAS-II peak fitting on diffraction images
# - Generates compressed Zarr datasets
# - Saves to Processed/{DateStamp}/{Sample}/Zarr/
#
# Usage:
#   1. Place recipe JSON files in ${HOME_DIR}/Params/recipes/
#   2. Edit the USER CONFIGURATION section below
#   3. Adjust node count and walltime based on dataset size
#   4. Submit: qsub scripts/submit_crux_production.pbs
#   5. Monitor: qstat -u $USER
#   6. Check output: tail -f xrd_prod_*.o*
#
# Scaling Guidelines:
#   Small datasets (100-500 frames):    4-8 nodes,   2-4 hours
#   Medium datasets (500-2000 frames):  16-32 nodes, 4-8 hours
#   Large datasets (2000-5000 frames):  32-64 nodes, 8-12 hours
#   Very large (5000+ frames):          64-128 nodes, 12-24 hours
#
# Expected speedup: ~75% linear scaling up to 64 nodes
#   (e.g., 32 nodes ≈ 24x faster than single node)
#
# Author: William Gonzalez
# Date: January 2025
################################################################################

#PBS -N xrd_production
#PBS -l select=32:system=crux
#PBS -l place=scatter
#PBS -l walltime=08:00:00
#PBS -l filesystems=home:eagle
#PBS -q workq-route
#PBS -A APS_INSITU_STUDY_APP
#PBS -j oe
#PBS -r y
#PBS -m abe
#PBS -M your.email@example.com

# ============================================================================
# USER CONFIGURATION - EDIT THESE VARIABLES
# ============================================================================

# Project allocation (REQUIRED - get from ALCF)
# Replace YourProjectName in #PBS -A above with your actual project name

# Email notifications (optional)
# Replace your.email@example.com in #PBS -M above to get job status emails
# -m abe: send email on (a)bort, (b)egin, (e)nd

# Processing parameters
# Place recipe JSON files in ${HOME_DIR}/Params/recipes/ before submission
# Recipes define: sample, images, peaks, frames, azimuths, etc.
# Use recipe_builder.py to create recipes or copy existing templates

# File paths on Crux - Project software directory
PROJECT_BASE="/eagle/APS_INSITU_STUDY_APP"
PROCESSOR_DIR="${PROJECT_BASE}/Software/PRISMA"
VENV_PATH="${PROJECT_BASE}/Software/venv"
GSAS2DIR="${PROJECT_BASE}/Software/GSAS-II"

# Home directory for XRD data structure
# This directory should contain: Data/, Processed/, Analysis/, Params/
# Data structure: Data/{MonthYear}/{Sample}/{Stage}/{Images,Refs}/
# Processed structure: Processed/{DateStamp}/{Sample}/{Zarr,Intensity}/
HOME_DIR="${PROJECT_BASE}"

# Parallel processing configuration
# Number of Dask workers per node (can be overridden at submission)
# Submit with custom value: qsub -v WORKERS_PER_NODE=128 scripts/submit_crux_production.pbs
# Or use wrapper: ./scripts/submit_crux.sh 128 production
#
# Memory per worker = ~256GB / WORKERS_PER_NODE
# GSAS-II uses 1-2GB per task, so aim for 2-4GB per worker
#
# Recommended values:
#   32  = conservative (~8GB/worker)
#   64  = balanced (~4GB/worker) [DEFAULT]
#   96  = aggressive (~2.5GB/worker)
#   128 = maximum (~2GB/worker)
WORKERS_PER_NODE=${WORKERS_PER_NODE:-64}  # Default: 64 workers per node

# Performance monitoring (set to 1 to enable detailed timing)
ENABLE_PROFILING=0

# ============================================================================
# SYSTEM CONFIGURATION (typically no changes needed)
# ============================================================================

# Load required modules (REQUIRED for MPI on Crux)
module load PrgEnv-gnu
module load cray-python/3.11.7

# Set up environment
cd "${PROCESSOR_DIR}" || exit 1

# HPC optimizations (prevent thread oversubscription)
# CRITICAL: Must be set BEFORE activating Python environment to prevent
# OpenBLAS from spawning too many threads during numpy initialization
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1

# Crux proxy settings (for compute nodes)
export http_proxy="http://proxy.alcf.anl.gov:3128"
export https_proxy="http://proxy.alcf.anl.gov:3128"

echo "======================================================================="
echo "Crux XRD Processing - PRODUCTION MODE"
echo "======================================================================="
echo "Job ID: ${PBS_JOBID}"
echo "Job Name: ${PBS_JOBNAME}"
echo "Queue: ${PBS_QUEUE}"
echo "Project: ${PBS_ACCOUNT}"
echo "Walltime requested: $(echo ${PBS_WALLTIME} | awk '{print $1/3600}') hours"
echo "Nodes requested: $(cat ${PBS_NODEFILE} | sort -u | wc -l)"
echo ""
echo "Node list:"
cat "${PBS_NODEFILE}" | sort -u
echo ""
echo "Start time: $(date '+%Y-%m-%d %H:%M:%S')"
echo "Working directory: $(pwd)"
echo "======================================================================="
echo ""

# Activate Python environment
echo "Activating Python environment..."
source "${VENV_PATH}/bin/activate"

# Configure PYTHONPATH for XRD package and GSAS-II
export PYTHONPATH="${PROCESSOR_DIR}:${GSAS2DIR}:${PYTHONPATH}"

# Dask configuration for large-scale processing (8K-16K workers)
export DASK_DISTRIBUTED__COMM__TIMEOUTS__CONNECT=600s              # 10 min (increased from 120s for massive scale)
export DASK_DISTRIBUTED__COMM__TIMEOUTS__TCP=600s                  # 10 min
export DASK_DISTRIBUTED__COMM__TIMEOUTS__SHUTDOWN=600s             # 10 min shutdown timeout
export DASK_DISTRIBUTED__SCHEDULER__WORKER_TTL=3600s               # 1 hour worker TTL
export DASK_DISTRIBUTED__SCHEDULER__ALLOWED_FAILURES=10            # Allow more transient failures
export DASK_DISTRIBUTED__WORKER__STARTUP_TIMEOUT=600s              # 10 min worker startup patience
export DASK_DISTRIBUTED__CORE__DEFAULT_CONNECT_TIMEOUT=600s        # 10 min default timeout
export DASK_DISTRIBUTED__WORKER__MEMORY__TARGET=0.70
export DASK_DISTRIBUTED__WORKER__MEMORY__SPILL=0.80
export DASK_DISTRIBUTED__WORKER__MEMORY__PAUSE=0.90
export DASK_DISTRIBUTED__WORKER__MEMORY__TERMINATE=0.95
export DASK_DISTRIBUTED__COMM__COMPRESSION=auto                     # auto (Crux requirement - lz4 doesn't work)
export DASK_DISTRIBUTED__SCHEDULER__IDLE_TIMEOUT=0                  # CRITICAL: Disable idle timeout for 8K+ worker initialization (prevents 120s shutdown)

# Profiling setup
if [ ${ENABLE_PROFILING} -eq 1 ]; then
    export DASK_DISTRIBUTED__WORKER__PROFILE__ENABLED=true
    export DASK_DISTRIBUTED__WORKER__PROFILE__INTERVAL=10s
    echo "Performance profiling: ENABLED"
else
    echo "Performance profiling: DISABLED"
fi

echo ""
echo "Environment configured:"
echo "  Python: $(which python)"
echo "  Python version: $(python --version)"
echo "  GSAS-II: ${GSAS2DIR}"
echo "  Home directory: ${HOME_DIR}"
echo ""

# System information
echo "System information:"
TOTAL_MEMORY=$(free -h | awk '/^Mem:/ {print $2}')
TOTAL_CORES=$(nproc)
echo "  Total memory: ${TOTAL_MEMORY}"
echo "  CPU cores per node: ${TOTAL_CORES}"
echo ""

# Verify critical imports
echo "Verifying Python packages..."
python -c "
import sys
import datetime
print(f'Verification time: {datetime.datetime.now()}')
print('Testing critical imports...')
try:
    import numpy as np
    print('✓ NumPy', np.__version__)
    import pandas as pd
    print('✓ Pandas', pd.__version__)
    import dask
    print('✓ Dask', dask.__version__)
    import dask_mpi
    print('✓ Dask-MPI available')
    from mpi4py import MPI
    print('✓ MPI4Py available')
    import zarr
    print('✓ Zarr', zarr.__version__)
    try:
        from zarr.codecs import BloscCodec
        print('✓ Zarr v3 BloscCodec (optimal compression)')
    except ImportError:
        print('⚠ Zarr v3 codecs not available (using v2 fallback)')
    # Try G2script shortcut first, fallback to direct import
    try:
        import G2script
        print('✓ GSAS-II (G2script shortcut)')
    except ImportError:
        from GSASII import GSASIIscriptable as G2script
        print('✓ GSAS-II (direct import)')
    import fabio
    # FabIO doesn't consistently expose __version__, try alternatives
    try:
        version = fabio.__version__
    except AttributeError:
        try:
            from importlib.metadata import version as get_version
            version = get_version('fabio')
        except Exception:
            version = 'installed'
    print('✓ FabIO', version)
    print('All imports successful!')
except ImportError as e:
    print('✗ Import failed:', e)
    sys.exit(1)
" || exit 1

echo ""
echo "======================================================================="
echo "Starting Dask-MPI Processing"
echo "======================================================================="
echo ""

# Get MPI configuration from PBS
NUM_NODES=$(cat ${PBS_NODEFILE} | sort -u | wc -l)
TOTAL_RANKS=$((NUM_NODES * WORKERS_PER_NODE))  # Workers per node × number of nodes
EXPECTED_WORKERS=$((TOTAL_RANKS - 2))  # Minus scheduler and client

echo "MPI Configuration:"
echo "  Nodes: ${NUM_NODES}"
echo "  Workers per node: ${WORKERS_PER_NODE}"
echo "  Total MPI ranks: ${TOTAL_RANKS}"
echo "  Expected Dask workers: ${EXPECTED_WORKERS} (scheduler + client use 2 ranks)"
echo ""
echo "Expected performance:"
if [ ${EXPECTED_WORKERS} -le 8 ]; then
    echo "  Speedup: ~${EXPECTED_WORKERS}x vs single worker"
elif [ ${EXPECTED_WORKERS} -le 256 ]; then
    SPEEDUP=$((EXPECTED_WORKERS * 75 / 100))
    echo "  Speedup: ~${SPEEDUP}x vs single worker (75% efficiency)"
else
    SPEEDUP=$((EXPECTED_WORKERS * 60 / 100))
    echo "  Speedup: ~${SPEEDUP}x vs single worker (60% efficiency)"
fi
echo ""

# Check for recipe files
RECIPE_COUNT=$(find "${HOME_DIR}/Params/recipes/" -maxdepth 1 -name "*.json" 2>/dev/null | wc -l)
if [ "${RECIPE_COUNT}" -eq 0 ]; then
    # Fallback to XRD/recipes if Params/recipes doesn't exist or is empty
    RECIPE_COUNT=$(find "${PROCESSOR_DIR}/XRD/recipes/" -maxdepth 1 -name "*.json" 2>/dev/null | wc -l)
    if [ "${RECIPE_COUNT}" -eq 0 ]; then
        echo "ERROR: No recipe JSON files found!"
        echo "Searched in:"
        echo "  ${HOME_DIR}/Params/recipes/"
        echo "  ${PROCESSOR_DIR}/XRD/recipes/"
        echo ""
        echo "Please create recipe files using recipe_builder.py"
        echo "See docs/CRUX_DEPLOYMENT.md for recipe format"
        exit 1
    fi
fi

echo "Found ${RECIPE_COUNT} recipe file(s) to process"
echo ""

# Start timing
START_TIME=$(date +%s)

echo "Launching XRD batch processing with Dask-MPI..."
echo "Time: $(date '+%Y-%m-%d %H:%M:%S')"
echo ""

# Execute batch processing with Dask-MPI
# mpiexec automatically distributes across nodes from PBS_NODEFILE
# Batch processor will:
# - Read recipe JSON files from Params/recipes/ or XRD/recipes/
# - Process diffraction images with GSAS-II peak fitting
# - Generate Zarr datasets in Processed/{DateStamp}/{Sample}/Zarr/
# - Move processed recipes to recipes/processed/
#
# MPI configuration:
# -n ${TOTAL_RANKS}        = Total processes across all nodes
# -ppn ${WORKERS_PER_NODE} = Processes per node (Dask workers)
# --cpu-bind verbose       = Show CPU binding for debugging
mpiexec -n ${TOTAL_RANKS} \
    -ppn ${WORKERS_PER_NODE} \
    --cpu-bind verbose \
    python XRD/processing/batch_processor.py --home "${HOME_DIR}"

EXIT_CODE=$?

# End timing
END_TIME=$(date +%s)
ELAPSED_TIME=$((END_TIME - START_TIME))
HOURS=$((ELAPSED_TIME / 3600))
MINUTES=$(((ELAPSED_TIME % 3600) / 60))
SECONDS=$((ELAPSED_TIME % 60))

echo ""
echo "======================================================================="
echo "Job Summary"
echo "======================================================================="
echo "Exit code: ${EXIT_CODE}"
echo "End time: $(date '+%Y-%m-%d %H:%M:%S')"
echo "Elapsed time: ${HOURS}h ${MINUTES}m ${SECONDS}s"
echo "Nodes used: ${NUM_NODES}"
echo "Workers: $((TOTAL_RANKS - 2))"

if [ ${EXIT_CODE} -eq 0 ]; then
    echo "Status: SUCCESS ✓"
    echo ""

    # Calculate performance metrics
    echo "Performance metrics:"
    echo "  Processing time: ${HOURS}h ${MINUTES}m ${SECONDS}s"
    echo "  Nodes used: ${NUM_NODES}"
    echo "  Workers: $((TOTAL_RANKS - 2))"
    echo "  Recipe files processed: ${RECIPE_COUNT}"
    echo ""

    echo "Output locations:"
    echo "  Zarr datasets: ${HOME_DIR}/Processed/<DateStamp>/<Sample>/Zarr/<ParamsString>/"
    echo "  Intensity plots: ${HOME_DIR}/Processed/<DateStamp>/<Sample>/Intensity/<ParamsString>/"
    echo "  Processed recipes: ${HOME_DIR}/Params/recipes/processed/ (or XRD/recipes/processed/)"
    echo ""

    echo "Next steps:"
    echo "  1. Check Zarr datasets: python XRD/tools/check_zarr.py <path-to-zarr>"
    echo "  2. Visualize data: python run_data_analyzer.py (GUI)"
    echo "  3. Transfer data: rsync from ${HOME_DIR}/Processed/"
    echo "  4. Review processing logs in recipes/processed/"

else
    echo "Status: FAILED ✗"
    echo ""
    echo "Troubleshooting:"
    echo "  1. Check error messages above"
    echo "  2. Verify recipe JSON files exist in ${HOME_DIR}/Params/recipes/"
    echo "  3. Check image paths in recipe files are valid"
    echo "  4. Verify GSAS-II calibration files (.imctrl, .immask) exist"
    echo "  5. Check data permissions and disk space"
    echo "  6. Review full output: cat ${HOME}/xrd_prod_${PBS_JOBID}.out"
    echo "  7. For help: docs/CRUX_DEPLOYMENT.md or contact support"
fi

echo "======================================================================="

# Archive log file
LOG_ARCHIVE="${HOME}/logs"
mkdir -p "${LOG_ARCHIVE}"
cp "${HOME}/xrd_prod_${PBS_JOBID}.out" "${LOG_ARCHIVE}/" 2>/dev/null || true

exit ${EXIT_CODE}
