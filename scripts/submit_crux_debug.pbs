#!/bin/bash
################################################################################
# Crux Debug Job Script
# ======================
# Quick testing job for XRD batch processing on ALCF Crux supercomputer
#
# Queue: debug (1-8 nodes, up to 2 hours)
# Purpose: Testing and development with fast turnaround
#
# This script processes recipe JSON files using Dask-MPI parallelization:
# - Reads recipes from Params/recipes/ or XRD/recipes/
# - Performs GSAS-II peak fitting on diffraction images
# - Generates compressed Zarr datasets
# - Saves to Processed/{DateStamp}/{Sample}/Zarr/
#
# Usage:
#   1. Place recipe JSON files in ${HOME_DIR}/Params/recipes/
#   2. Edit the USER CONFIGURATION section below
#   3. Submit: qsub scripts/submit_crux_debug.pbs
#   4. Monitor: qstat -u $USER
#   5. Check output: tail -f xrd_debug_*.o*
#
# Author: William Gonzalez
# Date: January 2025
################################################################################

#PBS -N xrd_debug
#PBS -l select=4:system=crux
#PBS -l place=scatter
#PBS -l walltime=02:00:00
#PBS -l filesystems=home:eagle
#PBS -q debug
#PBS -A APS_INSITU_STUDY_APP
#PBS -j oe
#PBS -r y

# ============================================================================
# USER CONFIGURATION - EDIT THESE VARIABLES
# ============================================================================

# Project allocation (REQUIRED - get from ALCF)
# Replace YourProjectName in #PBS -A above with your actual project name

# Processing parameters
# These are used for documentation/reference only
# Actual parameters are read from recipe JSON files
SAMPLE_NAME="5A2"
SETTING_NAME="30x50"
STAGE_NAME="BEF"

# File paths on Crux - Project software directory
PROJECT_BASE="/eagle/APS_INSITU_STUDY_APP"
PROCESSOR_DIR="${PROJECT_BASE}/Software/PRISMA"
VENV_PATH="${PROJECT_BASE}/Software/venv"
GSAS2DIR="${PROJECT_BASE}/Software/GSAS-II"

# Home directory for XRD data structure
# This directory should contain: Data/, Processed/, Analysis/, Params/
# Data structure: Data/{MonthYear}/{Sample}/{Stage}/{Images,Refs}/
# Processed structure: Processed/{DateStamp}/{Sample}/{Zarr,Intensity}/
HOME_DIR="${PROJECT_BASE}"

# Parallel processing configuration
# Number of Dask workers per node (can be overridden at submission)
# Submit with custom value: qsub -v WORKERS_PER_NODE=128 scripts/submit_crux_debug.pbs
# Or use wrapper: ./scripts/submit_crux.sh 128 debug
#
# Memory per worker = ~256GB / WORKERS_PER_NODE
# GSAS-II uses 1-2GB per task, so aim for 2-4GB per worker
#
# Recommended values:
#   32  = conservative (~8GB/worker)
#   64  = balanced (~4GB/worker) [DEFAULT]
#   96  = aggressive (~2.5GB/worker)
#   128 = maximum (~2GB/worker)
WORKERS_PER_NODE=${WORKERS_PER_NODE:-64}  # Default: 64 workers per node

# ============================================================================
# SYSTEM CONFIGURATION (typically no changes needed)
# ============================================================================

# Load required modules (REQUIRED for MPI on Crux)
module load PrgEnv-gnu
module load cray-python/3.11.7

# Set up environment
cd "${PROCESSOR_DIR}" || exit 1

# HPC optimizations (prevent thread oversubscription)
# CRITICAL: Must be set BEFORE activating Python environment to prevent
# OpenBLAS from spawning too many threads during numpy initialization
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1

# Crux proxy settings (for compute nodes)
export http_proxy="http://proxy.alcf.anl.gov:3128"
export https_proxy="http://proxy.alcf.anl.gov:3128"

echo "======================================================================="
echo "Crux XRD Processing - DEBUG MODE"
echo "======================================================================="
echo "Job ID: ${PBS_JOBID}"
echo "Job Name: ${PBS_JOBNAME}"
echo "Queue: ${PBS_QUEUE}"
echo "Nodes: $(cat ${PBS_NODEFILE} | wc -l)"
echo "Node list:"
cat "${PBS_NODEFILE}"
echo "Start time: $(date)"
echo "Working directory: $(pwd)"
echo "======================================================================="
echo ""

# Activate Python environment
echo "Activating Python environment..."
source "${VENV_PATH}/bin/activate"

# Configure PYTHONPATH for XRD package and GSAS-II
export PYTHONPATH="${PROCESSOR_DIR}:${GSAS2DIR}:${PYTHONPATH}"

# Dask configuration
export DASK_DISTRIBUTED__COMM__TIMEOUTS__CONNECT=60s
export DASK_DISTRIBUTED__COMM__TIMEOUTS__TCP=60s

echo "Environment configured:"
echo "  Python: $(which python)"
echo "  Python version: $(python --version)"
echo "  GSAS-II: ${GSAS2DIR}"
echo "  Home directory: ${HOME_DIR}"
echo ""

# Verify critical imports
echo "Verifying Python packages..."
python -c "
import sys
print('Testing critical imports...')
try:
    import numpy as np
    print('✓ NumPy', np.__version__)
    import dask
    print('✓ Dask', dask.__version__)
    import dask_mpi
    print('✓ Dask-MPI available')
    from mpi4py import MPI
    print('✓ MPI4Py available')
    import zarr
    print('✓ Zarr', zarr.__version__)
    # Try G2script shortcut first, fallback to direct import
    try:
        import G2script
        print('✓ GSAS-II (G2script shortcut)')
    except ImportError:
        from GSASII import GSASIIscriptable as G2script
        print('✓ GSAS-II (direct import)')
    print('All imports successful!')
except ImportError as e:
    print('✗ Import failed:', e)
    sys.exit(1)
" || exit 1

echo ""
echo "======================================================================="
echo "Starting Dask-MPI Processing"
echo "======================================================================="
echo ""

# Get MPI configuration from PBS
NUM_NODES=$(cat ${PBS_NODEFILE} | sort -u | wc -l)
TOTAL_RANKS=$((NUM_NODES * WORKERS_PER_NODE))  # Workers per node × number of nodes

echo "MPI Configuration:"
echo "  Nodes: ${NUM_NODES}"
echo "  Workers per node: ${WORKERS_PER_NODE}"
echo "  Total MPI ranks: ${TOTAL_RANKS}"
echo "  Expected Dask workers: $((TOTAL_RANKS - 2)) (scheduler + client use 2 ranks)"
echo ""

# Run batch processing with Dask-MPI
# Note: dask-mpi automatically sets up scheduler (1) + client (1) + workers (N-2)
# With 4 nodes: 1 scheduler + 1 client + 2 workers
#
# Batch processor will:
# - Read recipe JSON files from Params/recipes/ or XRD/recipes/
# - Process diffraction images with GSAS-II peak fitting
# - Generate Zarr datasets in Processed/{DateStamp}/{Sample}/Zarr/
# - Move processed recipes to recipes/processed/

echo "Launching XRD batch processing with Dask-MPI..."
echo "Expected configuration: 1 scheduler + 1 client + $((TOTAL_RANKS - 2)) workers"
echo ""

# Execute batch processing
# The --home parameter specifies the base directory for data structure
# (should contain Data/, Processed/, Params/ subdirectories)
#
# MPI configuration:
# -n ${TOTAL_RANKS}        = Total processes across all nodes
# -ppn ${WORKERS_PER_NODE} = Processes per node (Dask workers)
# --cpu-bind verbose       = Show CPU binding for debugging
mpiexec -n ${TOTAL_RANKS} \
    -ppn ${WORKERS_PER_NODE} \
    --cpu-bind verbose \
    python XRD/processing/batch_processor.py --home "${HOME_DIR}"

EXIT_CODE=$?

echo ""
echo "======================================================================="
echo "Job Summary"
echo "======================================================================="
echo "Exit code: ${EXIT_CODE}"
echo "End time: $(date)"

if [ ${EXIT_CODE} -eq 0 ]; then
    echo "Status: SUCCESS ✓"
    echo ""
    echo "Output files should be in:"
    echo "  ${HOME_DIR}/Processed/<DateStamp>/<Sample>/"
    echo "    - Zarr/<ParamsString>/ (processed data)"
    echo "    - Intensity/<ParamsString>/ (intensity plots per peak)"
    echo ""
    echo "Next steps:"
    echo "  1. Check Zarr datasets: use XRD/tools/check_zarr.py"
    echo "  2. Visualize data: use run_data_analyzer.py (GUI) or data_visualization.py"
    echo "  3. If successful, scale up with submit_crux_production.pbs"
    echo "  4. Transfer data: rsync from ${HOME_DIR}/Processed/"
else
    echo "Status: FAILED ✗"
    echo ""
    echo "Troubleshooting:"
    echo "  1. Check error messages above"
    echo "  2. Verify recipe JSON files exist in ${HOME_DIR}/Params/recipes/"
    echo "  3. Check image paths and GSAS-II calibration files in recipes"
    echo "  4. Verify data permissions and disk space"
    echo "  5. Review full output: cat ${HOME}/xrd_debug_${PBS_JOBID}.out"
fi

echo "======================================================================="

exit ${EXIT_CODE}
